{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_xnMOsbqHz61"
   },
   "source": [
    "# pix2pix: Image-to-image translation with a conditional GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pydot in /home/cgac/anaconda3/envs/agri/lib/python3.8/site-packages (1.4.2)\n",
      "Requirement already satisfied: pyparsing>=2.1.4 in /home/cgac/anaconda3/envs/agri/lib/python3.8/site-packages (from pydot) (2.4.7)\n"
     ]
    }
   ],
   "source": [
    "! pip install pydot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e1_Y75QXJS6h"
   },
   "source": [
    "## Import TensorFlow and other libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "YfIk2es3hJEd"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "import os\n",
    "import pathlib\n",
    "import time\n",
    "import datetime\n",
    "import pydot as pyd\n",
    "import keras\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from IPython import display\n",
    "\n",
    "keras.utils.vis_utils.pydot = pyd\n",
    "import cv2\n",
    "import numpy as np\n",
    "import uuid\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1fUzsnerj1P3"
   },
   "source": [
    "Each original image is of size `256 x 512` containing two `256 x 256` images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "XGY1kiptguTQ",
    "tags": []
   },
   "outputs": [],
   "source": [
    "PATH = '/home/cgac/Documents/agri-train-ds/'\n",
    "SUB_PATH = \"mask-all-in-one\"\n",
    "OUTPUT_GENERATOR= \"/home/cgac/Documents/agri-train-ds/mask_disease_classification_pix2pix/corns\"\n",
    "number_generate = 5\n",
    "# sample_image = tf.io.read_file(PATH  + 'train/1.jpg')\n",
    "# sample_image = tf.io.decode_jpeg(sample_image)\n",
    "# print(sample_image.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "id": "vJ2sO8Izg7QV",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# plt.figure()\n",
    "# plt.imshow(sample_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2A5SU-qxPAqd"
   },
   "source": [
    "You need to separate real building facade images from the architecture label images—all of which will be of size `256 x 256`.\n",
    "\n",
    "Define a function that loads image files and outputs two image tensors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "aO9ZAGH5K3SY",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load(image_file):\n",
    "    # Read and decode an image file to a uint8 tensor\n",
    "    image = tf.io.read_file(image_file)\n",
    "    image = tf.image.decode_jpeg(image)\n",
    "\n",
    "    # Split each image tensor into two tensors:\n",
    "    # - one with a real building facade image\n",
    "    # - one with an architecture label image \n",
    "    w = tf.shape(image)[1]\n",
    "    w =   w // 2\n",
    "    input_image = image[:, w:, :]\n",
    "    real_image = image[:, :w, :]\n",
    "\n",
    "#     print(real_image)\n",
    "    input_image =tf.image.resize(input_image, (256,256), preserve_aspect_ratio=False,antialias=False)\n",
    "    real_image = tf.image.resize(real_image, (256,256), preserve_aspect_ratio=False,antialias=False)\n",
    "    # Convert both images to float32 tensors\n",
    "    input_image = tf.cast(input_image, tf.float32)\n",
    "    real_image = tf.cast(real_image, tf.float32)\n",
    "\n",
    "    return input_image, real_image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r5ByHTlfE06P"
   },
   "source": [
    "Plot a sample of the input (architecture label image) and real (building facade photo) images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "id": "4OLHMpsQ5aOv",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# inp, re = load(PATH + 'train/1.jpg' )\n",
    "# # Casting to int for matplotlib to display the images\n",
    "# plt.figure()\n",
    "# plt.imshow(inp / 255.0)\n",
    "# plt.figure()\n",
    "# plt.imshow(re / 255.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PVuZQTfI_c-s"
   },
   "source": [
    "As described in the [pix2pix paper](https://arxiv.org/abs/1611.07004), you need to apply random jittering and mirroring to preprocess the training set.\n",
    "\n",
    "Define several functions that:\n",
    "\n",
    "1. Resize each `256 x 256` image to a larger height and width—`286 x 286`.\n",
    "2. Randomly crop it back to `256 x 256`.\n",
    "3. Randomly flip the image horizontally i.e. left to right (random mirroring).\n",
    "4. Normalize the images to the `[-1, 1]` range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "2CbTEt448b4R",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# The facade training set consist of 400 images\n",
    "BUFFER_SIZE = 400\n",
    "# The batch size of 1 produced better results for the U-Net in the original pix2pix experiment\n",
    "BATCH_SIZE = 1\n",
    "# Each image is 256x256 in size\n",
    "IMG_WIDTH = 256\n",
    "IMG_HEIGHT = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "rwwYQpu9FzDu",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def resize(input_image, real_image, height, width):\n",
    "  input_image = tf.image.resize(input_image, [height, width],\n",
    "                                method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
    "  real_image = tf.image.resize(real_image, [height, width],\n",
    "                               method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
    "\n",
    "  return input_image, real_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "Yn3IwqhiIszt",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def random_crop(input_image, real_image):\n",
    "  stacked_image = tf.stack([input_image, real_image], axis=0)\n",
    "  cropped_image = tf.image.random_crop(\n",
    "      stacked_image, size=[2, IMG_HEIGHT, IMG_WIDTH, 3])\n",
    "\n",
    "  return cropped_image[0], cropped_image[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "muhR2cgbLKWW",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Normalizing the images to [-1, 1]\n",
    "def normalize(input_image, real_image):\n",
    "  input_image = (input_image / 127.5) - 1\n",
    "  real_image = (real_image / 127.5) - 1\n",
    "\n",
    "  return input_image, real_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "fVQOjcPVLrUc",
    "tags": []
   },
   "outputs": [],
   "source": [
    "@tf.function()\n",
    "def random_jitter(input_image, real_image):\n",
    "  # Resizing to 286x286\n",
    "  input_image, real_image = resize(input_image, real_image, 286, 286)\n",
    "\n",
    "  # Random cropping back to 256x256\n",
    "  input_image, real_image = random_crop(input_image, real_image)\n",
    "\n",
    "  if tf.random.uniform(()) > 0.5:\n",
    "    # Random mirroring\n",
    "    input_image = tf.image.flip_left_right(input_image)\n",
    "    real_image = tf.image.flip_left_right(real_image)\n",
    "\n",
    "  return input_image, real_image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wfAQbzy799UV"
   },
   "source": [
    "You can inspect some of the preprocessed output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "n0OGdi6D92kM",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(6, 6))\n",
    "# for i in range(4):\n",
    "#   rj_inp, rj_re = random_jitter(inp, re)\n",
    "#   plt.subplot(2, 2, i + 1)\n",
    "#   plt.imshow(rj_inp / 255.0)\n",
    "#   plt.axis('off')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3E9LGq3WBmsh"
   },
   "source": [
    "Having checked that the loading and preprocessing works, let's define a couple of helper functions that load and preprocess the training and test sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "tyaP4hLJ8b4W",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_image_train(image_file):\n",
    "  input_image, real_image = load(image_file)\n",
    "  input_image, real_image = random_jitter(input_image, real_image)\n",
    "  input_image, real_image = normalize(input_image, real_image)\n",
    "\n",
    "  return input_image, real_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "VB3Z6D_zKSru",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_image_test(image_file):\n",
    "  input_image, real_image = load(image_file)\n",
    "  input_image, real_image = resize(input_image, real_image,\n",
    "                                   IMG_HEIGHT, IMG_WIDTH)\n",
    "  input_image, real_image = normalize(input_image, real_image)\n",
    "\n",
    "  return input_image, real_image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PIGN6ouoQxt3"
   },
   "source": [
    "## Build an input pipeline with `tf.data`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SQHmYSmk8b4b",
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "PATH_IMG = pathlib.Path(PATH)\n",
    "# PATH_IMG = list(PATH_IMG.glob('*.*'))\n",
    "# print(PATH_IMG)\n",
    "train_dataset = tf.data.Dataset.list_files( str(PATH_IMG / f'{SUB_PATH}/*.*'))\n",
    "\n",
    "\n",
    "train_dataset = train_dataset.map(load_image_train,\n",
    "                                  num_parallel_calls=tf.data.AUTOTUNE)\n",
    "train_dataset = train_dataset.shuffle(BUFFER_SIZE)\n",
    "train_dataset = train_dataset.batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in  train_dataset.take(1):\n",
    "#     print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "THY-sZMiQ4UV"
   },
   "source": [
    "## Build the generator\n",
    "\n",
    "The generator of your pix2pix cGAN is a _modified_ [U-Net](https://arxiv.org/abs/1505.04597). A U-Net consists of an encoder (downsampler) and decoder (upsampler). (You can find out more about it in the [Image segmentation](https://www.tensorflow.org/tutorials/images/segmentation) tutorial and on the [U-Net project website](https://lmb.informatik.uni-freiburg.de/people/ronneber/u-net/).)\n",
    "\n",
    "- Each block in the encoder is: Convolution -> Batch normalization -> Leaky ReLU\n",
    "- Each block in the decoder is: Transposed convolution -> Batch normalization -> Dropout (applied to the first 3 blocks) -> ReLU\n",
    "- There are skip connections between the encoder and decoder (as in the U-Net)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4MQPuBCgtldI"
   },
   "source": [
    "Define the downsampler (encoder):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "tqqvWxlw8b4l",
    "tags": []
   },
   "outputs": [],
   "source": [
    "OUTPUT_CHANNELS = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "3R09ATE_SH9P",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def downsample(filters, size, apply_batchnorm=True):\n",
    "  initializer = tf.random_normal_initializer(0., 0.02)\n",
    "\n",
    "  result = tf.keras.Sequential()\n",
    "  result.add(\n",
    "      tf.keras.layers.Conv2D(filters, size, strides=2, padding='same',\n",
    "                             kernel_initializer=initializer, use_bias=False))\n",
    "\n",
    "  if apply_batchnorm:\n",
    "    result.add(tf.keras.layers.BatchNormalization())\n",
    "\n",
    "  result.add(tf.keras.layers.LeakyReLU())\n",
    "\n",
    "  return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "id": "a6_uCZCppTh7",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# down_model = downsample(3, 4)\n",
    "# down_result = down_model(tf.expand_dims(inp, 0))\n",
    "# print (down_result.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aFI_Pa52tjLl"
   },
   "source": [
    "Define the upsampler (decoder):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "nhgDsHClSQzP",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def upsample(filters, size, apply_dropout=False):\n",
    "  initializer = tf.random_normal_initializer(0., 0.02)\n",
    "\n",
    "  result = tf.keras.Sequential()\n",
    "  result.add(\n",
    "    tf.keras.layers.Conv2DTranspose(filters, size, strides=2,\n",
    "                                    padding='same',\n",
    "                                    kernel_initializer=initializer,\n",
    "                                    use_bias=False))\n",
    "\n",
    "  result.add(tf.keras.layers.BatchNormalization())\n",
    "\n",
    "  if apply_dropout:\n",
    "      result.add(tf.keras.layers.Dropout(0.5))\n",
    "\n",
    "  result.add(tf.keras.layers.ReLU())\n",
    "\n",
    "  return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "id": "mz-ahSdsq0Oc",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# up_model = upsample(3, 4)\n",
    "# up_result = up_model(down_result)\n",
    "# print (up_result.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ueEJyRVrtZ-p"
   },
   "source": [
    "Define the generator with the downsampler and the upsampler:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "lFPI4Nu-8b4q",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def Generator():\n",
    "  inputs = tf.keras.layers.Input(shape=[256, 256, 3])\n",
    "\n",
    "  down_stack = [\n",
    "    downsample(64, 4, apply_batchnorm=False),  # (batch_size, 128, 128, 64)\n",
    "    downsample(128, 4),  # (batch_size, 64, 64, 128)\n",
    "    downsample(256, 4),  # (batch_size, 32, 32, 256)\n",
    "    downsample(512, 4),  # (batch_size, 16, 16, 512)\n",
    "    downsample(512, 4),  # (batch_size, 8, 8, 512)\n",
    "    downsample(512, 4),  # (batch_size, 4, 4, 512)\n",
    "    downsample(512, 4),  # (batch_size, 2, 2, 512)\n",
    "    downsample(512, 4),  # (batch_size, 1, 1, 512)\n",
    "  ]\n",
    "\n",
    "  up_stack = [\n",
    "    upsample(512, 4, apply_dropout=True),  # (batch_size, 2, 2, 1024)\n",
    "    upsample(512, 4, apply_dropout=True),  # (batch_size, 4, 4, 1024)\n",
    "    upsample(512, 4, apply_dropout=True),  # (batch_size, 8, 8, 1024)\n",
    "    upsample(512, 4),  # (batch_size, 16, 16, 1024)\n",
    "    upsample(256, 4),  # (batch_size, 32, 32, 512)\n",
    "    upsample(128, 4),  # (batch_size, 64, 64, 256)\n",
    "    upsample(64, 4),  # (batch_size, 128, 128, 128)\n",
    "  ]\n",
    "\n",
    "  initializer = tf.random_normal_initializer(0., 0.02)\n",
    "  last = tf.keras.layers.Conv2DTranspose(OUTPUT_CHANNELS, 4,\n",
    "                                         strides=2,\n",
    "                                         padding='same',\n",
    "                                         kernel_initializer=initializer,\n",
    "                                         activation='tanh')  # (batch_size, 256, 256, 3)\n",
    "\n",
    "  x = inputs\n",
    "\n",
    "  # Downsampling through the model\n",
    "  skips = []\n",
    "  for down in down_stack:\n",
    "    x = down(x)\n",
    "    skips.append(x)\n",
    "\n",
    "  skips = reversed(skips[:-1])\n",
    "\n",
    "  # Upsampling and establishing the skip connections\n",
    "  for up, skip in zip(up_stack, skips):\n",
    "    x = up(x)\n",
    "    x = tf.keras.layers.Concatenate()([x, skip])\n",
    "\n",
    "  x = last(x)\n",
    "\n",
    "  return tf.keras.Model(inputs=inputs, outputs=x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z4PKwrcQFYvF"
   },
   "source": [
    "Visualize the generator model architecture:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dIbRPFzjmV85",
    "tags": []
   },
   "outputs": [],
   "source": [
    "generator = Generator()\n",
    "len(generator.layers)\n",
    "# tf.keras.utils.plot_model(generator, show_shapes=True, dpi=64,to_file='pix2pix_figure/gen_net.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(tf.newaxis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z8kbgTK8FcPo"
   },
   "source": [
    "Test the generator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "id": "U1N1_obwtdQH",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# gen_output = generator(inp[tf.newaxis, ...], training=False)\n",
    "# plt.imshow(gen_output[0, ...])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dpDPEQXIAiQO"
   },
   "source": [
    "### Define the generator loss\n",
    "\n",
    "GANs learn a loss that adapts to the data, while cGANs learn a structured loss that penalizes a possible structure that differs from the network output and the target image, as described in the [pix2pix paper](https://arxiv.org/abs/1611.07004).\n",
    "\n",
    "- The generator loss is a sigmoid cross-entropy loss of the generated images and an **array of ones**.\n",
    "- The pix2pix paper also mentions the L1 loss, which is a MAE (mean absolute error) between the generated image and the target image.\n",
    "- This allows the generated image to become structurally similar to the target image.\n",
    "- The formula to calculate the total generator loss is `gan_loss + LAMBDA * l1_loss`, where `LAMBDA = 100`. This value was decided by the authors of the paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "id": "cyhxTuvJyIHV",
    "tags": []
   },
   "outputs": [],
   "source": [
    "LAMBDA = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "id": "Q1Xbz5OaLj5C",
    "tags": []
   },
   "outputs": [],
   "source": [
    "loss_object = tf.keras.losses.BinaryCrossentropy(from_logits=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "id": "90BIcCKcDMxz",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generator_loss(disc_generated_output, gen_output, target):\n",
    "  gan_loss = loss_object(tf.ones_like(disc_generated_output), disc_generated_output)\n",
    "\n",
    "  # Mean absolute error\n",
    "  l1_loss = tf.reduce_mean(tf.abs(target - gen_output))\n",
    "\n",
    "  total_gen_loss = gan_loss + (LAMBDA * l1_loss)\n",
    "\n",
    "  return total_gen_loss, gan_loss, l1_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fSZbDgESHIV6"
   },
   "source": [
    "The training procedure for the generator is as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZTKZfoaoEF22"
   },
   "source": [
    "## Build the discriminator\n",
    "\n",
    "The discriminator in the pix2pix cGAN is a convolutional PatchGAN classifier—it tries to classify if each image _patch_ is real or not real, as described in the [pix2pix paper](https://arxiv.org/abs/1611.07004).\n",
    "\n",
    "- Each block in the discriminator is: Convolution -> Batch normalization -> Leaky ReLU.\n",
    "- The shape of the output after the last layer is `(batch_size, 30, 30, 1)`.\n",
    "- Each `30 x 30` image patch of the output classifies a `70 x 70` portion of the input image.\n",
    "- The discriminator receives 2 inputs: \n",
    "    - The input image and the target image, which it should classify as real.\n",
    "    - The input image and the generated image (the output of the generator), which it should classify as fake.\n",
    "    - Use `tf.concat([inp, tar], axis=-1)` to concatenate these 2 inputs together."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XIuTeGL5v45m"
   },
   "source": [
    "Let's define the discriminator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "id": "ll6aNeQx8b4v",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def Discriminator():\n",
    "  initializer = tf.random_normal_initializer(0., 0.02)\n",
    "\n",
    "  inp = tf.keras.layers.Input(shape=[256, 256, 3], name='input_image')\n",
    "  tar = tf.keras.layers.Input(shape=[256, 256, 3], name='target_image')\n",
    "\n",
    "  x = tf.keras.layers.concatenate([inp, tar])  # (batch_size, 256, 256, channels*2)\n",
    "\n",
    "  down1 = downsample(64, 4, False)(x)  # (batch_size, 128, 128, 64)\n",
    "  down2 = downsample(128, 4)(down1)  # (batch_size, 64, 64, 128)\n",
    "  down3 = downsample(256, 4)(down2)  # (batch_size, 32, 32, 256)\n",
    "\n",
    "  zero_pad1 = tf.keras.layers.ZeroPadding2D()(down3)  # (batch_size, 34, 34, 256)\n",
    "  conv = tf.keras.layers.Conv2D(512, 4, strides=1,\n",
    "                                kernel_initializer=initializer,\n",
    "                                use_bias=False)(zero_pad1)  # (batch_size, 31, 31, 512)\n",
    "\n",
    "  batchnorm1 = tf.keras.layers.BatchNormalization()(conv)\n",
    "\n",
    "  leaky_relu = tf.keras.layers.LeakyReLU()(batchnorm1)\n",
    "\n",
    "  zero_pad2 = tf.keras.layers.ZeroPadding2D()(leaky_relu)  # (batch_size, 33, 33, 512)\n",
    "\n",
    "  last = tf.keras.layers.Conv2D(1, 4, strides=1,\n",
    "                                kernel_initializer=initializer)(zero_pad2)  # (batch_size, 30, 30, 1)\n",
    "\n",
    "  return tf.keras.Model(inputs=[inp, tar], outputs=last)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HdV9yAbBHNkg"
   },
   "source": [
    "Visualize the discriminator model architecture:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "id": "YHoUui4om-Ev",
    "tags": []
   },
   "outputs": [],
   "source": [
    "discriminator = Discriminator()\n",
    "# tf.keras.utils.plot_model(discriminator, show_shapes=True, dpi=64,to_file='pix2pix_figure/dis_net.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ps7nIHigHYc7"
   },
   "source": [
    "Test the discriminator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "id": "gDkA05NE6QMs",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# disc_out = discriminator([inp[tf.newaxis, ...], gen_output], training=False)\n",
    "# plt.imshow(disc_out[0, ..., -1], vmin=-20, vmax=20, cmap='RdBu_r')\n",
    "# plt.colorbar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AOqg1dhUAWoD"
   },
   "source": [
    "### Define the discriminator loss\n",
    "\n",
    "- The `discriminator_loss` function takes 2 inputs: **real images** and **generated images**.\n",
    "- `real_loss` is a sigmoid cross-entropy loss of the **real images** and an **array of ones(since these are the real images)**.\n",
    "- `generated_loss` is a sigmoid cross-entropy loss of the **generated images** and an **array of zeros (since these are the fake images)**.\n",
    "- The `total_loss` is the sum of `real_loss` and `generated_loss`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "id": "wkMNfBWlT-PV",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def discriminator_loss(disc_real_output, disc_generated_output):\n",
    "  real_loss = loss_object(tf.ones_like(disc_real_output), disc_real_output)\n",
    "\n",
    "  generated_loss = loss_object(tf.zeros_like(disc_generated_output), disc_generated_output)\n",
    "\n",
    "  total_disc_loss = real_loss + generated_loss\n",
    "\n",
    "  return total_disc_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-ede4p2YELFa"
   },
   "source": [
    "The training procedure for the discriminator is shown below.\n",
    "\n",
    "To learn more about the architecture and the hyperparameters you can refer to the [pix2pix paper](https://arxiv.org/abs/1611.07004)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0FMYgY_mPfTi"
   },
   "source": [
    "## Define the optimizers and a checkpoint-saver\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "id": "lbHFNexF0x6O",
    "tags": []
   },
   "outputs": [],
   "source": [
    "generator_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\n",
    "discriminator_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "id": "WJnftd5sQsv6",
    "tags": []
   },
   "outputs": [],
   "source": [
    "checkpoint_dir = './pix2pix_mask_all_in_one_logs'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "checkpoint = tf.train.Checkpoint(generator_optimizer=generator_optimizer,\n",
    "                                 discriminator_optimizer=discriminator_optimizer,\n",
    "                                 generator=generator,\n",
    "                                 discriminator=discriminator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rw1fkAczTQYh"
   },
   "source": [
    "## Generate images\n",
    "\n",
    "Write a function to plot some images during training.\n",
    "\n",
    "- Pass images from the test set to the generator.\n",
    "- The generator will then translate the input image into the output.\n",
    "- The last step is to plot the predictions and _voila_!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rb0QQFHF-JfS"
   },
   "source": [
    "Note: The `training=True` is intentional here since\n",
    "you want the batch statistics, while running the model on the test dataset. If you use `training=False`, you get the accumulated statistics learned from the training dataset (which you don't want)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "id": "RmdVsmvhPxyy",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import random\n",
    "from PIL import Image\n",
    "def generate_images(model, test_input, tar):\n",
    "    if not os.path.exists(OUTPUT_GENERATOR+ f\"/{SUB_PATH}\"):\n",
    "        FULL_DIR = OUTPUT_GENERATOR + \"/\" + SUB_PATH\n",
    "        print(\"ful dir\",FULL_DIR)\n",
    "        os.makedirs(FULL_DIR)\n",
    "    for i in range(number_generate):\n",
    "        uique = uuid.uuid4()\n",
    "        fname = f\"{OUTPUT_GENERATOR}/{SUB_PATH}/{SUB_PATH}_{uique}.jpg\"\n",
    "        prediction = model(test_input, training=True)\n",
    "        img = (prediction[0].numpy()*0.5+0.5) * 255 # increasing value to be in 0-255 \n",
    "        img = cv2.cvtColor(img, cv2.COLOR_RGBA2BGR)\n",
    "        is_save = cv2.imwrite(fname,img)\n",
    "        print(\"save \", is_save)\n",
    "#         plt.imshow((prediction[0]*0.5+0.5) )\n",
    "#         plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kz80bY3aQ1VZ"
   },
   "source": [
    "## Restore the latest checkpoint and test the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "id": "HSSm4kfvJiqv",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checkpoint\t\t     ckpt-21.data-00000-of-00001\n",
      "ckpt-10.data-00000-of-00001  ckpt-21.index\n",
      "ckpt-10.index\t\t     ckpt-22.data-00000-of-00001\n",
      "ckpt-11.data-00000-of-00001  ckpt-22.index\n",
      "ckpt-11.index\t\t     ckpt-23.data-00000-of-00001\n",
      "ckpt-12.data-00000-of-00001  ckpt-23.index\n",
      "ckpt-12.index\t\t     ckpt-24.data-00000-of-00001\n",
      "ckpt-13.data-00000-of-00001  ckpt-24.index\n",
      "ckpt-13.index\t\t     ckpt-2.data-00000-of-00001\n",
      "ckpt-14.data-00000-of-00001  ckpt-2.index\n",
      "ckpt-14.index\t\t     ckpt-3.data-00000-of-00001\n",
      "ckpt-15.data-00000-of-00001  ckpt-3.index\n",
      "ckpt-15.index\t\t     ckpt-4.data-00000-of-00001\n",
      "ckpt-16.data-00000-of-00001  ckpt-4.index\n",
      "ckpt-16.index\t\t     ckpt-5.data-00000-of-00001\n",
      "ckpt-17.data-00000-of-00001  ckpt-5.index\n",
      "ckpt-17.index\t\t     ckpt-6.data-00000-of-00001\n",
      "ckpt-18.data-00000-of-00001  ckpt-6.index\n",
      "ckpt-18.index\t\t     ckpt-7.data-00000-of-00001\n",
      "ckpt-19.data-00000-of-00001  ckpt-7.index\n",
      "ckpt-19.index\t\t     ckpt-8.data-00000-of-00001\n",
      "ckpt-1.data-00000-of-00001   ckpt-8.index\n",
      "ckpt-1.index\t\t     ckpt-9.data-00000-of-00001\n",
      "ckpt-20.data-00000-of-00001  ckpt-9.index\n",
      "ckpt-20.index\t\t     fit\n"
     ]
    }
   ],
   "source": [
    "!ls {checkpoint_dir}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "id": "4t4x69adQ5xb",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7fced5042d90>"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Restoring the latest checkpoint in checkpoint_dir\n",
    "checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1RGysMU_BZhx"
   },
   "source": [
    "## Generate some images using the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "id": "KUgSnmy2nqSP",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ful dir /home/cgac/Documents/agri-train-ds/mask_disease_classification_pix2pix/corns/corn_gray_leaf\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n",
      "save  True\n"
     ]
    }
   ],
   "source": [
    "# Run the trained model on a few examples from the test set\n",
    "# as_numpy_iterator\n",
    "for inp, tar in train_dataset.as_numpy_iterator():\n",
    "    generate_images(generator, inp, tar)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "pix2pix.ipynb",
   "private_outputs": true,
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
